{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "###Hyper-parameter-search"
   ],
   "metadata": {
    "id": "e5_sbSTGpl-I"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zxCfl-Spev7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "train_pilot_df = pd.read_csv(\"data/training.csv\")\n",
    "train_pilot_df[\"label\"] = train_pilot_df[\"label\"].map(LABEL2ID)\n",
    "\n",
    "assert train_pilot_df[\"label\"].isna().sum() == 0\n",
    "\n",
    "\n",
    "pilot_df, _ = train_test_split(\n",
    "    train_pilot_df,\n",
    "    train_size=0.1,\n",
    "    stratify=train_pilot_df[\"label\"],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pilot_ds = Dataset.from_pandas(pilot_df[[\"sentence\",\"label\"]])\n",
    "pilot_ds = pilot_ds.map(tokenize, batched=True)\n",
    "pilot_ds = pilot_ds.remove_columns([\"sentence\"])\n",
    "pilot_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "lr_min, lr_max = 5e-6, 3e-5\n",
    "wd_min, wd_max = 0.0, 0.05\n",
    "bs_choices = [8, 16]\n",
    "scheduler_choices= [\"cosine\", \"linear\"]\n",
    "warmup_min, warmup_max = 0.0, 0.2\n",
    "\n",
    "N = 30\n",
    "random.seed(seed); np.random.seed(seed)\n",
    "configs = []\n",
    "for _ in range(N):\n",
    "    lr = 10**random.uniform(math.log10(lr_min), math.log10(lr_max))\n",
    "    wd = random.uniform(wd_min, wd_max)\n",
    "    bs = random.choice(bs_choices)\n",
    "    sched = random.choice(scheduler_choices)\n",
    "    warm  = random.uniform(warmup_min, warmup_max)\n",
    "    configs.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": wd,\n",
    "        \"per_device_train_batch_size\": bs,\n",
    "        \"lr_scheduler_type\": sched,\n",
    "        \"warmup_ratio\": warm\n",
    "    })\n",
    "\n",
    "def run_pilot(config, idx):\n",
    "    args = arguments.copy()\n",
    "    args.update({\n",
    "        \"learning_rate\": config[\"learning_rate\"],\n",
    "        \"weight_decay\": config[\"weight_decay\"],\n",
    "        \"per_device_train_batch_size\": config[\"per_device_train_batch_size\"],\n",
    "        \"lr_scheduler_type\": config[\"lr_scheduler_type\"],\n",
    "        \"warmup_ratio\": config[\"warmup_ratio\"],\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"output_dir\": f\"pilot/run_{idx}\",\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"eval_strategy\": \"epoch\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"load_best_model_at_end\": False,\n",
    "    })\n",
    "    pilot_args = TrainingArguments(**args)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=3, id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=pilot_args,\n",
    "        train_dataset=pilot_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    history = trainer.state.log_history\n",
    "    train_losses = [log[\"loss\"] for log in history if \"loss\" in log and \"eval_loss\" not in log]\n",
    "    train_loss = train_losses[-1] if train_losses else None\n",
    "\n",
    "    return {\n",
    "        **config,\n",
    "        \"train_loss\":    train_loss,\n",
    "        \"eval_loss\":     metrics.get(\"eval_loss\"),\n",
    "        \"eval_accuracy\": metrics.get(\"eval_accuracy\"),\n",
    "        \"eval_f1\":       metrics.get(\"eval_f1\"),\n",
    "        \"eval_mae\":      metrics.get(\"eval_mae\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "results = []\n",
    "for idx, cfg in enumerate(configs, 1):\n",
    "    print(f\"→ Pilot {idx}/{N}: {cfg}\")\n",
    "    res = run_pilot(cfg, idx)\n",
    "    results.append(res)\n",
    "    print(\n",
    "        f\"   train_loss={res['train_loss']:.4f}, \"\n",
    "        f\"eval_loss={res['eval_loss']:.4f}, \"\n",
    "        f\"acc={res['eval_accuracy']:.4f}, \"\n",
    "        f\"f1={res['eval_f1']:.4f}, \"\n",
    "        f\"mae={res['eval_mae']:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "# Build DataFrame and sort by MAE ascending\n",
    "df = pd.DataFrame(results)\n",
    "df_sorted = df.sort_values(\"eval_mae\").reset_index(drop=True)\n",
    "df_sorted"
   ],
   "metadata": {
    "id": "hVpYtzxkpzm_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Assume df_sorted from the previous pilot sweep is in scope\n",
    "top5 = df_sorted.head(5)\n",
    "\n",
    "results_2ep = []\n",
    "\n",
    "for idx, row in top5.iterrows():\n",
    "    # Extract config\n",
    "    config = {\n",
    "        \"learning_rate\":               row[\"learning_rate\"],\n",
    "        \"weight_decay\":                row[\"weight_decay\"],\n",
    "        \"per_device_train_batch_size\": int(row[\"per_device_train_batch_size\"]),\n",
    "        \"lr_scheduler_type\":           row[\"lr_scheduler_type\"],\n",
    "        \"warmup_ratio\":                row[\"warmup_ratio\"],\n",
    "    }\n",
    "    print(f\"→ Top-5 run {idx+1}: {config}\")\n",
    "\n",
    "    # Build TrainingArguments for 2 epochs\n",
    "    args = arguments.copy()\n",
    "    args.update({\n",
    "        \"learning_rate\":            config[\"learning_rate\"],\n",
    "        \"weight_decay\":             config[\"weight_decay\"],\n",
    "        \"per_device_train_batch_size\": config[\"per_device_train_batch_size\"],\n",
    "        \"lr_scheduler_type\":        config[\"lr_scheduler_type\"],\n",
    "        \"warmup_ratio\":             config[\"warmup_ratio\"],\n",
    "        \"num_train_epochs\":         2,\n",
    "        \"output_dir\":               f\"pilot/top5_run_{idx+1}\",\n",
    "        \"eval_strategy\":      \"epoch\",\n",
    "        \"save_strategy\":            \"no\",\n",
    "        \"load_best_model_at_end\":   False,\n",
    "        \"logging_steps\":            100,\n",
    "    })\n",
    "    two_epoch_args = TrainingArguments(**args)\n",
    "\n",
    "    # Initialize fresh model & trainer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=3, id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=two_epoch_args,\n",
    "        train_dataset=pilot_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # Train for 2 epochs and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Extract final train loss from log history\n",
    "    history = trainer.state.log_history\n",
    "    train_losses = [log[\"loss\"] for log in history if \"loss\" in log and \"eval_loss\" not in log]\n",
    "    train_loss = train_losses[-1] if train_losses else None\n",
    "\n",
    "    # Record results\n",
    "    results_2ep.append({\n",
    "        **config,\n",
    "        \"train_loss\":    train_loss,\n",
    "        \"eval_loss\":     metrics.get(\"eval_loss\"),\n",
    "        \"eval_accuracy\": metrics.get(\"eval_accuracy\"),\n",
    "        \"eval_f1\":       metrics.get(\"eval_f1\"),\n",
    "        \"eval_mae\":      metrics.get(\"eval_mae\"),\n",
    "    })\n",
    "    print(\n",
    "        f\"   train_loss={train_loss:.4f}, \"\n",
    "        f\"eval_loss={metrics['eval_loss']:.4f}, \"\n",
    "        f\"acc={metrics['eval_accuracy']:.4f}, \"\n",
    "        f\"f1={metrics['eval_f1']:.4f}, \"\n",
    "        f\"mae={metrics['eval_mae']:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "# Display summary table\n",
    "df_2ep = pd.DataFrame(results_2ep)\n",
    "df_2ep_sorted = df_2ep.sort_values(\"eval_mae\").reset_index(drop=True)\n",
    "df_2ep_sorted"
   ],
   "metadata": {
    "id": "nYCOH89Ep2mH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# 1) Extract top-2 configs from the 2-epoch results\n",
    "top2 = df_2ep_sorted.head(2).copy().reset_index(drop=True)\n",
    "\n",
    "# 2) Define the seeds to test\n",
    "seeds_to_test = [13, 23]\n",
    "\n",
    "# 3) Prepare a list to collect all runs\n",
    "stability_results = []\n",
    "\n",
    "for idx, row in top2.iterrows():\n",
    "    config = {\n",
    "        \"learning_rate\": row[\"learning_rate\"],\n",
    "        \"weight_decay\": row[\"weight_decay\"],\n",
    "        \"per_device_train_batch_size\": int(row[\"per_device_train_batch_size\"]),\n",
    "        \"lr_scheduler_type\": row[\"lr_scheduler_type\"],\n",
    "        \"warmup_ratio\": row[\"warmup_ratio\"],\n",
    "    }\n",
    "    print(f\"→ Config #{idx+1}: {config}\")\n",
    "\n",
    "    for seed_val in seeds_to_test:\n",
    "        # 4) Reseed RNGs for reproducible init\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "        set_seed(seed_val)\n",
    "\n",
    "        # 5) Build TrainingArguments for 2 epochs\n",
    "        args = arguments.copy()\n",
    "        args.update({\n",
    "            \"learning_rate\":            config[\"learning_rate\"],\n",
    "            \"weight_decay\":             config[\"weight_decay\"],\n",
    "            \"per_device_train_batch_size\": config[\"per_device_train_batch_size\"],\n",
    "            \"lr_scheduler_type\":        config[\"lr_scheduler_type\"],\n",
    "            \"warmup_ratio\":             config[\"warmup_ratio\"],\n",
    "            \"num_train_epochs\":         2,\n",
    "            \"output_dir\":               f\"pilot/top2_cfg{idx+1}_seed{seed_val}\",\n",
    "            \"eval_strategy\":      \"epoch\",\n",
    "            \"save_strategy\":            \"no\",\n",
    "            \"load_best_model_at_end\":   False,\n",
    "            \"logging_steps\":            100,\n",
    "        })\n",
    "        seed_args = TrainingArguments(**args)\n",
    "\n",
    "        # 6) Init fresh model & trainer\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=3, id2label=ID2LABEL, label2id=LABEL2ID\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=seed_args,\n",
    "            train_dataset=pilot_ds,\n",
    "            eval_dataset=pilot_ds,     # evaluating on same pilot slice\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=collator,\n",
    "        )\n",
    "\n",
    "        # 7) Train & evaluate\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        # extract train_loss from log history\n",
    "        history = trainer.state.log_history\n",
    "        train_losses = [log[\"loss\"] for log in history if \"loss\" in log and \"eval_loss\" not in log]\n",
    "        train_loss = train_losses[-1] if train_losses else None\n",
    "\n",
    "        stability_results.append({\n",
    "            \"cfg_idx\":      idx+1,\n",
    "            \"seed\":         seed_val,\n",
    "            **config,\n",
    "            \"train_loss\":   train_loss,\n",
    "            \"eval_loss\":    metrics.get(\"eval_loss\"),\n",
    "            \"eval_accuracy\":metrics.get(\"eval_accuracy\"),\n",
    "            \"eval_f1\":      metrics.get(\"eval_f1\"),\n",
    "            \"eval_mae\":     metrics.get(\"eval_mae\"),\n",
    "        })\n",
    "        print(f\"   seed={seed_val} → mae={metrics['eval_mae']:.4f}\")\n",
    "\n",
    "# 8) Build a DataFrame and compute mean±std per config\n",
    "stability_df = pd.DataFrame(stability_results)\n",
    "summary = stability_df.groupby(\"cfg_idx\").agg(\n",
    "    lr     = (\"learning_rate\", \"first\"),\n",
    "    wd     = (\"weight_decay\", \"first\"),\n",
    "    bs     = (\"per_device_train_batch_size\", \"first\"),\n",
    "    sched  = (\"lr_scheduler_type\", \"first\"),\n",
    "    warmup = (\"warmup_ratio\", \"first\"),\n",
    "    mae_mean = (\"eval_mae\", \"mean\"),\n",
    "    mae_std  = (\"eval_mae\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "summary"
   ],
   "metadata": {
    "id": "SEumcjKGp55l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 1) Extract the winning config\n",
    "best_cfg = {\n",
    "    \"learning_rate\":               2.02e-05,\n",
    "    \"weight_decay\":                0.0191,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"lr_scheduler_type\":           \"cosine\",\n",
    "    \"warmup_ratio\":                0.10,\n",
    "}\n",
    "\n",
    "# 2) Build TrainingArguments for a single epoch on full data\n",
    "args = arguments.copy()\n",
    "args.update({\n",
    "    \"learning_rate\":             best_cfg[\"learning_rate\"],\n",
    "    \"weight_decay\":              best_cfg[\"weight_decay\"],\n",
    "    \"per_device_train_batch_size\": best_cfg[\"per_device_train_batch_size\"],\n",
    "    \"lr_scheduler_type\":         best_cfg[\"lr_scheduler_type\"],\n",
    "    \"warmup_ratio\":              best_cfg[\"warmup_ratio\"],\n",
    "    \"num_train_epochs\":          1,\n",
    "    \"output_dir\":                \"sanity/full_data_cfg2\",\n",
    "    \"eval_strategy\":       \"epoch\",\n",
    "    \"save_strategy\":             \"no\",\n",
    "    \"load_best_model_at_end\":    False,\n",
    "    \"logging_steps\":             500,\n",
    "})\n",
    "sanity_args = TrainingArguments(**args)\n",
    "\n",
    "# 3) Initialize model & trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=3, id2label=ID2LABEL, label2id=LABEL2ID\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sanity_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# 4) Train and evaluate\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "id": "jHQJ5UoRp8zy"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
