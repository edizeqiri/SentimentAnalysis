{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ────────────── standard lib ──────────────\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "\n",
        "# ────────────── scientific stack ──────────────\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ────────────── plotting ──────────────\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ────────────── scikit-learn ──────────────\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# ────────────── utilities ──────────────\n",
        "import scipy.special  # for scipy.special.softmax"
      ],
      "metadata": {
        "id": "gVlAq_xco66i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SoIYLTLaoWrl"
      },
      "outputs": [],
      "source": [
        "########## EDITABLE PARAMS ##########\n",
        "RUN_ID      = \"logreg_bow_baseline\"\n",
        "TRAIN_DATA_PATH   = \"data/training_split.csv\"\n",
        "VAL_DATA_PATH   = \"data/validation_split.csv\"\n",
        "SEEDS       = [13, 21, 42]           # -- three independent passes\n",
        "BATCH_SIZE  = 32                     # only used for timing parity\n",
        "MAX_FEATURES = 10_000\n",
        "NGRAM_RANGE  = (1, 2)\n",
        "C            = 1.0                  # LR inverse-reg strength\n",
        "MAX_ITER     = 300\n",
        "#####################################\n",
        "\n",
        "OUT_ROOT = Path(f\"results/{RUN_ID}\")\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
        "val_df   = pd.read_csv(VAL_DATA_PATH)\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label2id).astype(\"int64\")\n",
        "val_df[\"label\"]   = val_df[\"label\"].map(label2id).astype(\"int64\")"
      ],
      "metadata": {
        "id": "wLHLLh-qoglr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    # 3-a  reproducible randomness\n",
        "    np.random.seed(seed); random.seed(seed)\n",
        "\n",
        "    # 3-b  vectoriser + model for this seed\n",
        "    vectoriser = CountVectorizer(\n",
        "        ngram_range=NGRAM_RANGE,\n",
        "        max_features=MAX_FEATURES\n",
        "    )\n",
        "    X_train = vectoriser.fit_transform(train_df[\"sentence\"])\n",
        "    X_val   = vectoriser.transform(val_df[\"sentence\"])\n",
        "\n",
        "    y_train = train_df[\"label\"].values\n",
        "    y_val   = val_df[\"label\"].values\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        C=C, max_iter=MAX_ITER,\n",
        "        random_state=seed,              # <<< makes coef_ reproducible\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    tic = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    logits = model.decision_function(X_val)      # shape (N, 3)\n",
        "    latency = time.time() - tic"
      ],
      "metadata": {
        "id": "qJMXDGEnoj-H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba = scipy.special.softmax(logits, axis=1)\n",
        "p_raw = proba.dot(np.arange(3))          # float in [0,2]\n",
        "p_int = np.argmax(proba, axis=1)         # or model.predict(...)\n",
        "y_int = y_val"
      ],
      "metadata": {
        "id": "FqvwqbRgomJ1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae   = np.abs(p_raw - y_int).mean()\n",
        "score = 0.5 * (2 - mae)\n",
        "acc   = accuracy_score(y_int, p_int)\n",
        "\n",
        "rec = {\n",
        "    \"seed\": seed,\n",
        "    \"score\":        float(score),\n",
        "    \"mae\":          float(mae),\n",
        "    \"accuracy\":     float(acc),\n",
        "    \"latency_sec\":  float(latency),\n",
        "}\n",
        "records.append(rec)\n",
        "\n",
        "# --- file outputs ---\n",
        "out_dir = OUT_ROOT / f\"seed_{seed}\"\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "json.dump(rec, open(out_dir/\"metrics.json\", \"w\"), indent=2)\n",
        "\n",
        "cm = confusion_matrix(y_int, p_int, labels=[0,1,2], normalize=\"true\")\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "sns.heatmap(cm, annot=True, fmt=\".2f\",\n",
        "            xticklabels=list(label2id), yticklabels=list(label2id), ax=ax)\n",
        "ax.set_title(f\"{RUN_ID} | seed {seed}\")\n",
        "fig.tight_layout(); fig.savefig(out_dir/\"confusion_matrix.png\", dpi=200)\n",
        "plt.close(fig)\n",
        "\n",
        "mis = val_df.iloc[np.where(p_int != y_int)[0]][[\"id\",\"sentence\",\"label\"]]\n",
        "mis[\"pred\"] = [id2label[i] for i in p_int[p_int != y_int]]\n",
        "mis[\"label\"] = mis[\"label\"].map(id2label)\n",
        "mis.to_csv(out_dir/\"misclassified.csv\", index=False)"
      ],
      "metadata": {
        "id": "rhWJoxhroocW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rec = pd.DataFrame(records)\n",
        "\n",
        "# parameter counting\n",
        "n_params_total = model.coef_.size + model.intercept_.size   # all trainable\n",
        "n_params_total_M = round(n_params_total / 1_000_000, 2)\n",
        "\n",
        "agg = {\n",
        "    \"score_mean\":    df_rec[\"score\"].mean(),\n",
        "    \"score_std\":     df_rec[\"score\"].std(ddof=0),\n",
        "    \"mae_mean\":      df_rec[\"mae\"].mean(),\n",
        "    \"mae_std\":       df_rec[\"mae\"].std(ddof=0),\n",
        "    \"latency_sec_mean\": df_rec[\"latency_sec\"].mean(),\n",
        "    \"latency_sec_std\":  df_rec[\"latency_sec\"].std(ddof=0),\n",
        "    \"params_M_total\":     n_params_total_M,\n",
        "    \"params_M_trainable\": n_params_total_M,   # all weights are trained\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"max_features\": MAX_FEATURES\n",
        "}\n",
        "json.dump(agg, open(OUT_ROOT/\"aggregate.json\", \"w\"), indent=2)\n",
        "print(agg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Mp_eZJopQF",
        "outputId": "3ab51616-e029-42cb-edee-d408637eae37"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score_mean': np.float64(0.7798940194147028), 'score_std': 0.0, 'mae_mean': np.float64(0.4402119611705944), 'mae_std': 0.0, 'latency_sec_mean': np.float64(6.768828392028809), 'latency_sec_std': 0.0, 'params_M_total': 0.03, 'params_M_trainable': 0.03, 'batch_size': 32, 'max_features': 10000}\n"
          ]
        }
      ]
    }
  ]
}