{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0a14XX0nOXx",
    "outputId": "bd9f8505-d656-4973-c952-59685e5ea2a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "import random\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_synonyms(word):\n",
    "    syns = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lem in syn.lemmas():\n",
    "            w = lem.name().replace('_',' ').lower()\n",
    "            if w != word:\n",
    "                syns.add(w)\n",
    "    return list(syns)\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    if not words:\n",
    "        return words\n",
    "    new_words = words.copy()\n",
    "    candidates = [w for w in words if w not in STOP_WORDS]\n",
    "    random.shuffle(candidates)\n",
    "    num_replaced = 0\n",
    "    for w in candidates:\n",
    "        syns = get_synonyms(w)\n",
    "        if syns:\n",
    "            new_words = [random.choice(syns) if x == w else x for x in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return new_words\n",
    "\n",
    "def random_insertion(words, n):\n",
    "    if not words:\n",
    "        return words\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        candidates = [w for w in new_words if w not in STOP_WORDS]\n",
    "        if not candidates:\n",
    "            break\n",
    "        w = random.choice(candidates)\n",
    "        syns = get_synonyms(w)\n",
    "        if not syns:\n",
    "            continue\n",
    "        insert_word = random.choice(syns)\n",
    "        idx = random.randint(0, len(new_words))\n",
    "        new_words.insert(idx, insert_word)\n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    if len(words) < 2:\n",
    "        return words\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        i, j = random.sample(range(len(new_words)), 2)\n",
    "        new_words[i], new_words[j] = new_words[j], new_words[i]\n",
    "    return new_words\n",
    "\n",
    "def random_deletion(words, p):\n",
    "    if len(words) <= 1:\n",
    "        return words\n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    return new_words if new_words else [random.choice(words)]\n",
    "\n",
    "def eda(sentence, alpha=0.1, n_aug=4):\n",
    "    words = sentence.split()\n",
    "    l = len(words)\n",
    "    if l == 0:\n",
    "        return []\n",
    "    n = max(1, int(alpha * l))\n",
    "    augmented = []\n",
    "    ops = []\n",
    "    if any(get_synonyms(w) for w in words):\n",
    "        ops.append(lambda w: synonym_replacement(w, n))\n",
    "    if len(words) >= 1:\n",
    "        ops.append(lambda w: random_insertion(w, n))\n",
    "    if len(words) >= 2:\n",
    "        ops.append(lambda w: random_swap(w, n))\n",
    "    ops.append(lambda w: random_deletion(w, alpha))\n",
    "\n",
    "    for _ in range(n_aug):\n",
    "        op = random.choice(ops)\n",
    "        aug_words = op(words)\n",
    "        augmented.append(\" \".join(aug_words))\n",
    "    return augmented\n",
    "\n",
    "train_df = pd.read_csv(\"data/training_split.csv\")\n",
    "alpha= 0.1\n",
    "n_aug= 4\n",
    "aug_sentences, aug_labels = [], []\n",
    "for sent, lbl in zip(train_df[\"sentence\"], train_df[\"label\"]):\n",
    "    seen = set([sent])\n",
    "    aug_count = 0\n",
    "    while aug_count < n_aug:\n",
    "        samples = eda(sent, alpha=alpha, n_aug=1)\n",
    "        if not samples:\n",
    "            aug_count += 1\n",
    "            break\n",
    "        aug = samples[0]\n",
    "        if aug not in seen:\n",
    "            aug_sentences.append(aug)\n",
    "            aug_labels.append(lbl)\n",
    "            seen.add(aug)\n",
    "        aug_count += 1\n",
    "\n",
    "aug_df = pd.DataFrame({\"sentence\": aug_sentences, \"label\": aug_labels})\n",
    "full_train = pd.concat([train_df, aug_df], ignore_index=True)\n",
    "full_train.to_csv(\"data/training_split_eda.csv\", index=False)\n",
    "\n",
    "print(f\"Original samples: {len(train_df)}\")\n",
    "print(f\"Augmented samples: {len(aug_df)}\")\n",
    "print(f\"Total samples saved: {len(full_train)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc3s4cBRnVIc",
    "outputId": "fa643127-ac1d-481f-a5f4-97684587d25e"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original samples: 91887\n",
      "Augmented samples: 302148\n",
      "Total samples saved: 394035\n"
     ]
    }
   ]
  }
 ]
}
