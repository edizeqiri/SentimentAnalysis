{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0a14XX0nOXx",
        "outputId": "bd9f8505-d656-4973-c952-59685e5ea2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# ──────────────────────────────────────────\n",
        "# Cell 1: Install & configure NLTK/WordNet\n",
        "# ──────────────────────────────────────────\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# load stop‐word set once\n",
        "STOP_WORDS = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────\n",
        "# Cell 2 (updated): Define safe EDA functions & augment train set\n",
        "# ──────────────────────────────────────────\n",
        "import copy\n",
        "import random\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "def get_synonyms(word):\n",
        "    syns = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lem in syn.lemmas():\n",
        "            w = lem.name().replace('_',' ').lower()\n",
        "            if w != word:\n",
        "                syns.add(w)\n",
        "    return list(syns)\n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "    if not words:\n",
        "        return words\n",
        "    new_words = words.copy()\n",
        "    candidates = [w for w in words if w not in STOP_WORDS]\n",
        "    random.shuffle(candidates)\n",
        "    num_replaced = 0\n",
        "    for w in candidates:\n",
        "        syns = get_synonyms(w)\n",
        "        if syns:\n",
        "            new_words = [random.choice(syns) if x == w else x for x in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return new_words\n",
        "\n",
        "def random_insertion(words, n):\n",
        "    if not words:\n",
        "        return words\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        candidates = [w for w in new_words if w not in STOP_WORDS]\n",
        "        if not candidates:\n",
        "            break\n",
        "        w = random.choice(candidates)\n",
        "        syns = get_synonyms(w)\n",
        "        if not syns:\n",
        "            continue\n",
        "        insert_word = random.choice(syns)\n",
        "        idx = random.randint(0, len(new_words))\n",
        "        new_words.insert(idx, insert_word)\n",
        "    return new_words\n",
        "\n",
        "def random_swap(words, n):\n",
        "    if len(words) < 2:\n",
        "        return words\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        i, j = random.sample(range(len(new_words)), 2)\n",
        "        new_words[i], new_words[j] = new_words[j], new_words[i]\n",
        "    return new_words\n",
        "\n",
        "def random_deletion(words, p):\n",
        "    if len(words) <= 1:\n",
        "        return words\n",
        "    new_words = [w for w in words if random.random() > p]\n",
        "    return new_words if new_words else [random.choice(words)]\n",
        "\n",
        "def eda(sentence, alpha=0.1, n_aug=4):\n",
        "    \"\"\"\n",
        "    Perform EDA (Wei & Zou, 2019) on one sentence.\n",
        "      alpha: percent of words to change for SR/RI/RS or deletion prob for RD\n",
        "      n_aug: how many augmented samples to produce\n",
        "    Returns: list of n_aug augmented sentences (strings).\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    l = len(words)\n",
        "    if l == 0:\n",
        "        return []\n",
        "    # number of edits per operation\n",
        "    n = max(1, int(alpha * l))\n",
        "    augmented = []\n",
        "    ops = []\n",
        "    # only include ops that make sense\n",
        "    if any(get_synonyms(w) for w in words):\n",
        "        ops.append(lambda w: synonym_replacement(w, n))\n",
        "    if len(words) >= 1:\n",
        "        ops.append(lambda w: random_insertion(w, n))\n",
        "    if len(words) >= 2:\n",
        "        ops.append(lambda w: random_swap(w, n))\n",
        "    ops.append(lambda w: random_deletion(w, alpha))\n",
        "\n",
        "    for _ in range(n_aug):\n",
        "        op = random.choice(ops)\n",
        "        aug_words = op(words)\n",
        "        augmented.append(\" \".join(aug_words))\n",
        "    return augmented\n",
        "\n",
        "# ──────────────────────────────────────────\n",
        "# Now load your training data, augment it, and save\n",
        "# ──────────────────────────────────────────\n",
        "train_df = pd.read_csv(\"data/training_split.csv\")   # [\"sentence\",\"label\"]\n",
        "alpha    = 0.1    # ~10% of words changed\n",
        "n_aug    = 4      # 3 new sentences per original\n",
        "\n",
        "aug_sentences, aug_labels = [], []\n",
        "for sent, lbl in zip(train_df[\"sentence\"], train_df[\"label\"]):\n",
        "    seen = set([sent])        # track originals + any duplicates\n",
        "    aug_count = 0\n",
        "    while aug_count < n_aug:\n",
        "        samples = eda(sent, alpha=alpha, n_aug=1)\n",
        "        if not samples:\n",
        "            aug_count += 1\n",
        "            break\n",
        "        aug = samples[0]\n",
        "        if aug not in seen:\n",
        "            aug_sentences.append(aug)\n",
        "            aug_labels.append(lbl)\n",
        "            seen.add(aug)\n",
        "        aug_count += 1\n",
        "\n",
        "aug_df = pd.DataFrame({\"sentence\": aug_sentences, \"label\": aug_labels})\n",
        "full_train = pd.concat([train_df, aug_df], ignore_index=True)\n",
        "full_train.to_csv(\"data/training_split_eda.csv\", index=False)\n",
        "\n",
        "print(f\"Original samples: {len(train_df)}\")\n",
        "print(f\"Augmented samples: {len(aug_df)}\")\n",
        "print(f\"Total samples saved: {len(full_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc3s4cBRnVIc",
        "outputId": "fa643127-ac1d-481f-a5f4-97684587d25e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original samples: 91887\n",
            "Augmented samples: 302148\n",
            "Total samples saved: 394035\n"
          ]
        }
      ]
    }
  ]
}